{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "## Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/nfs/lab/varpred/Results/biobank_imbal_and_peaks\"\n",
    "t1d_mat_imbalance_binary = file_path + '/T1D_mat_imbalance_binary_val.txt'\n",
    "t1d_mat_peaks_binary = file_path + '/T1D_mat_peaks_binary_val.txt'\n",
    "t1d_mat_imbalance_actual = file_path + '/T1D_mat_imbalance_actual_val.txt'\n",
    "t1d_mat_peaks_actual = file_path + '/T1D_mat_peaks_actual_val.txt'\n",
    "t2d_mat_imbalance_binary = file_path + '/T2D_mat_imbalance_binary_val.txt'\n",
    "t2d_mat_peaks_binary = file_path + '/T2D_mat_peaks_binary_val.txt'\n",
    "t2d_mat_imbalance_actual = file_path + '/T2D_mat_imbalance_actual_val.txt'\n",
    "t2d_mat_peaks_actual = file_path + '/T2D_mat_peaks_actual_val.txt'\n",
    "proba_threshold = 0.7\n"
   ]
  },
  {
   "source": [
    "# Data Ingestion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1d_imbalance_binary = pandas.read_csv(t1d_mat_imbalance_binary, sep=' ')\n",
    "t1d_peaks_binary = pandas.read_csv(t1d_mat_peaks_binary, sep=' ')\n",
    "t1d_imbalance_actual = pandas.read_csv(t1d_mat_imbalance_actual, sep=' ')\n",
    "t1d_peaks_actual = pandas.read_csv(t1d_mat_peaks_actual, sep=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1d_peaks_actual = t1d_peaks_actual.rename(columns={\"endo\": \"endocrine\"})\n",
    "# t1d_peaks_binary = t1d_peaks_binary.rename(columns={\"endo\": \"endocrine\"})\n",
    "\n",
    "cell_types = list(t1d_imbalance_actual.columns)[2:]\n",
    "raw_data_frames = {\n",
    "    \"imbalance_binary\": t1d_imbalance_binary,\n",
    "    \"peaks_binary\": t1d_peaks_binary,\n",
    "    \"imbalance_actual\": t1d_imbalance_actual,\n",
    "    \"peaks_actual\": t1d_peaks_actual,\n",
    "}\n",
    "\n",
    "data = pandas.concat([raw_data_frames['imbalance_actual'], raw_data_frames['imbalance_binary'], raw_data_frames['peaks_actual'], raw_data_frames['peaks_binary']], axis=1)\n",
    "labels = data['probability'].iloc[:,0]\n",
    "data = data.drop(['variant_id', 'probability'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = t1d_imbalance_binary['probability']\n",
    "\n",
    "# Categorical labelling\n",
    "def label_prob(prob):\n",
    "    if prob > proba_threshold:\n",
    "        return 1\n",
    "    elif prob < 1 - proba_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "labels = labels.apply(lambda row: label_prob(row))"
   ]
  },
  {
   "source": [
    "# Label Propagation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prop_model = LabelPropagation()\n",
    "# label_prop_model.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(label_prop_model, open(f\"./models/label_prop_model_thresh_{proba_threshold}\", 'wb'))"
   ]
  },
  {
   "source": [
    "# Cross Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out unlabelled data\n",
    "known_data_bool = labels != -1\n",
    "data.insert(len(data.columns), 'label', labels)\n",
    "data.insert(len(data.columns), 'known', known_data_bool)\n",
    "all_known_data = data[data.known == True]\n",
    "known_data = all_known_data.drop(['label', 'known'], axis=1)\n",
    "known_labels = all_known_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(label_prop_model, known_data, y=known_labels, verbose=1, n_jobs=4)"
   ]
  },
  {
   "source": [
    "# Label Spreading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_spread_model = LabelSpreading(alpha=(1 - proba_threshold))\n",
    "# label_spread_model.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(label_spread_model, open(f\"./models/label_spread_model_thresh_{proba_threshold}\", 'wb'))"
   ]
  },
  {
   "source": [
    "# Experimenting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_data_bool = labels != -1\n",
    "all_data = data.copy(deep=True)\n",
    "all_data.insert(len(all_data.columns), 'label', labels)\n",
    "all_data.insert(len(all_data.columns), 'known', known_data_bool)\n",
    "all_known_data = all_data[all_data.known == True]\n",
    "all_unknown_data = all_data[all_data.known == False]\n",
    "known_data = all_known_data.drop(['label', 'known'], axis=1)\n",
    "known_labels = all_known_data['label']\n",
    "unknown_data = all_unknown_data.drop(['label', 'known'], axis=1)\n",
    "unknown_labels = all_unknown_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num unknown: 61\nNum negative: 89104\nNum positive: 21\n"
     ]
    }
   ],
   "source": [
    "print('Num unknown:', len(unknown_labels))\n",
    "print('Num negative:', len(known_labels[known_labels == 0]))\n",
    "print('Num positive:', len(known_labels[known_labels == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
    "splits = ss.split(all_known_data)\n",
    "\n",
    "def custom_splitter(splits):\n",
    "    print(type(splits))\n",
    "    print(len(list(unknown_data.index.values)))\n",
    "    for train_index, test_index in splits:\n",
    "        print(\"%s %s\" % (len(train_index), len(test_index)))\n",
    "        train_index = np.append(train_index, list(unknown_data.index.values))\n",
    "        print(\"%s %s\" % (len(train_index), len(test_index)))\n",
    "        yield train_index, test_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'generator'>\n",
      "61\n",
      "66843 22282\n",
      "66904 22282\n",
      "66843 22282\n",
      "66904 22282\n",
      "66843 22282\n",
      "66904 22282\n",
      "66843 22282\n",
      "66904 22282\n",
      "66843 22282\n",
      "66904 22282\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed: 59.0min finished\n"
     ]
    }
   ],
   "source": [
    "cv_score = cross_val_score(label_prop_model, data, y=labels, cv=custom_splitter(splits), verbose=1, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.99910241, 0.99883314, 0.99923705, 0.99941657, 0.99928193])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'generator'>\n",
      "61\n",
      "66843 22282\n",
      "66904 22282\n",
      "66843 22282\n",
      "66904 22282\n",
      "66843 22282\n",
      "66904 22282\n",
      "66843 22282\n",
      "66904 22282\n",
      "66843 22282\n",
      "66904 22282\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 20.8min finished\n"
     ]
    }
   ],
   "source": [
    "cv_score = cross_val_score(label_spread_model, data, y=labels, cv=custom_splitter(splits), verbose=1, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.99910241, 0.99883314, 0.99923705, 0.99941657, 0.99928193])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}